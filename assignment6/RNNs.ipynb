{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zieqYUrGRhDa",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {}
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "outputId": "051f3637-d80f-4993-c5ce-70a2bfcc010c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "outputId": "d8b8e064-5e2e-4270-a54b-f8d85a9fa713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJhwnrz0M1TO",
        "colab_type": "code",
        "outputId": "5a005ced-6a1d-4a23-c1ba-de71729ae670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "' '.join([word for word,tag in data[0]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FlHpqGpBnlk",
        "colab_type": "code",
        "outputId": "feaf2f28-a9e7-4b95-896f-ab67a3a32aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "outputId": "978a53f8-d9c5-4286-eeed-c7739b76f094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "outputId": "5234f416-1914-424e-d1e3-06580d8eb4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'PRT', 'PRON', 'NOUN', 'DET', 'ADV', 'ADJ', 'ADP', 'VERB', 'NUM', 'X', '.', 'CONJ'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOh2D4G1wOm5",
        "colab_type": "code",
        "outputId": "cbb60859-7e3a-4140-fc12-fcebde25ec7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2ind['passion']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5ois29cBlRe",
        "colab_type": "code",
        "outputId": "507575c7-2299-46bd-c2a7-3214da347872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2ind['room']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33147"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "outputId": "9c1c9927-8e1f-43f7-cd7a-39b84da689e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "outputId": "c66c8a4e-b031-42e6-e789-230f050ece93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "outputId": "edba7cf9-9040-491b-94d7-13eaa09d4540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "outputId": "c4e02ecb-056c-4b9b-c304-13a541199aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q1WhakUCx9C",
        "colab_type": "code",
        "outputId": "374fca71-5c96-4585-f551-7bdc3b0f2aa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(X_train).head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26695</td>\n",
              "      <td>29413.0</td>\n",
              "      <td>18176.0</td>\n",
              "      <td>29269.0</td>\n",
              "      <td>28273.0</td>\n",
              "      <td>7568.0</td>\n",
              "      <td>15857.0</td>\n",
              "      <td>20245.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>17300.0</td>\n",
              "      <td>33545.0</td>\n",
              "      <td>17555.0</td>\n",
              "      <td>12965.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>17217.0</td>\n",
              "      <td>40730.0</td>\n",
              "      <td>13891.0</td>\n",
              "      <td>37813.0</td>\n",
              "      <td>37813.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>36695</td>\n",
              "      <td>17341.0</td>\n",
              "      <td>14391.0</td>\n",
              "      <td>12965.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>22945.0</td>\n",
              "      <td>36953.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>11091.0</td>\n",
              "      <td>1861.0</td>\n",
              "      <td>27083.0</td>\n",
              "      <td>22876.0</td>\n",
              "      <td>13328.0</td>\n",
              "      <td>22298.0</td>\n",
              "      <td>23269.0</td>\n",
              "      <td>17542.0</td>\n",
              "      <td>36953.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>40773.0</td>\n",
              "      <td>6217.0</td>\n",
              "      <td>29205.0</td>\n",
              "      <td>29539.0</td>\n",
              "      <td>34769.0</td>\n",
              "      <td>36953.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>36826.0</td>\n",
              "      <td>2931.0</td>\n",
              "      <td>37890.0</td>\n",
              "      <td>1255.0</td>\n",
              "      <td>5757.0</td>\n",
              "      <td>13328.0</td>\n",
              "      <td>36953.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>34929.0</td>\n",
              "      <td>35574.0</td>\n",
              "      <td>6678.0</td>\n",
              "      <td>32301.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29909</td>\n",
              "      <td>26485.0</td>\n",
              "      <td>23361.0</td>\n",
              "      <td>18123.0</td>\n",
              "      <td>35314.0</td>\n",
              "      <td>26822.0</td>\n",
              "      <td>4259.0</td>\n",
              "      <td>22298.0</td>\n",
              "      <td>14513.0</td>\n",
              "      <td>21661.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>28420.0</td>\n",
              "      <td>23572.0</td>\n",
              "      <td>31256.0</td>\n",
              "      <td>37765.0</td>\n",
              "      <td>12965.0</td>\n",
              "      <td>5604.0</td>\n",
              "      <td>36953.0</td>\n",
              "      <td>13414.0</td>\n",
              "      <td>3382.0</td>\n",
              "      <td>5689.0</td>\n",
              "      <td>39879.0</td>\n",
              "      <td>22519.0</td>\n",
              "      <td>27083.0</td>\n",
              "      <td>23550.0</td>\n",
              "      <td>28234.0</td>\n",
              "      <td>23550.0</td>\n",
              "      <td>4259.0</td>\n",
              "      <td>3919.0</td>\n",
              "      <td>25913.0</td>\n",
              "      <td>1229.0</td>\n",
              "      <td>19548.0</td>\n",
              "      <td>1067.0</td>\n",
              "      <td>27083.0</td>\n",
              "      <td>28234.0</td>\n",
              "      <td>31784.0</td>\n",
              "      <td>30104.0</td>\n",
              "      <td>1067.0</td>\n",
              "      <td>27083.0</td>\n",
              "      <td>33545.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>41978</td>\n",
              "      <td>3856.0</td>\n",
              "      <td>23977.0</td>\n",
              "      <td>36205.0</td>\n",
              "      <td>16146.0</td>\n",
              "      <td>20440.0</td>\n",
              "      <td>23205.0</td>\n",
              "      <td>16457.0</td>\n",
              "      <td>3856.0</td>\n",
              "      <td>44841.0</td>\n",
              "      <td>32301.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26695</td>\n",
              "      <td>22021.0</td>\n",
              "      <td>38387.0</td>\n",
              "      <td>13350.0</td>\n",
              "      <td>13891.0</td>\n",
              "      <td>27083.0</td>\n",
              "      <td>1328.0</td>\n",
              "      <td>33065.0</td>\n",
              "      <td>32301.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 161 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0        1        2        3        4    ...  156  157  158  159  160\n",
              "0  26695  29413.0  18176.0  29269.0  28273.0  ...  NaN  NaN  NaN  NaN  NaN\n",
              "1  36695  17341.0  14391.0  12965.0  23205.0  ...  NaN  NaN  NaN  NaN  NaN\n",
              "2  29909  26485.0  23361.0  18123.0  35314.0  ...  NaN  NaN  NaN  NaN  NaN\n",
              "3  41978   3856.0  23977.0  36205.0  16146.0  ...  NaN  NaN  NaN  NaN  NaN\n",
              "4  26695  22021.0  38387.0  13350.0  13891.0  ...  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "[5 rows x 161 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjxswYOCH5xt",
        "colab_type": "code",
        "outputId": "31bed402-060f-4d66-cc18-ebddfd0f856c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "pd.DataFrame(y_train).head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>3.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 161 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3     4     5    6    ...  154  155  156  157  158  159  160\n",
              "0   11  4.0  8.0  2.0   8.0   7.0  3.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
              "1    7  4.0  3.0  7.0   4.0   6.0  7.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
              "2    4  3.0  8.0  5.0   8.0   8.0  7.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
              "3    5  6.0  3.0  5.0   8.0   7.0  4.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
              "4   11  2.0  8.0  5.0  11.0  11.0  8.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN\n",
              "\n",
              "[5 rows x 161 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqjjQUIhHsIt",
        "colab_type": "code",
        "outputId": "743af2c0-1aed-403c-ace5-4098ff498684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for w in word2ind.keys():\n",
        "  if word2ind[w] == 4558:\n",
        "    print(w)\n",
        "    break\n",
        "word2ind[w]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stein\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4558"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgVAGs-9IoG7",
        "colab_type": "code",
        "outputId": "89eb3964-4bea-46be-e565-8d95df1ab1dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tag2ind"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 11,\n",
              " '<pad>': 0,\n",
              " 'ADJ': 6,\n",
              " 'ADP': 7,\n",
              " 'ADV': 5,\n",
              " 'CONJ': 12,\n",
              " 'DET': 4,\n",
              " 'NOUN': 3,\n",
              " 'NUM': 9,\n",
              " 'PRON': 2,\n",
              " 'PRT': 1,\n",
              " 'VERB': 8,\n",
              " 'X': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdRISUUnDSOo",
        "colab_type": "code",
        "outputId": "9df5e653-77ea-4881-9471-73911c9f167e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train[0][:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[26695, 29413, 18176, 29269, 28273]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOV37yzMF6_C",
        "colab_type": "code",
        "outputId": "e87a1fea-1295-4f4a-ce32-c9f0d80f7654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('``', '.'),\n",
              " ('What', 'DET'),\n",
              " ('did', 'VERB'),\n",
              " ('you', 'PRON'),\n",
              " ('think', 'VERB'),\n",
              " ('about', 'ADP'),\n",
              " (\"Bang-Jensen's\", 'NOUN'),\n",
              " ('contention', 'NOUN'),\n",
              " ('of', 'ADP'),\n",
              " ('errors', 'NOUN'),\n",
              " ('and', 'CONJ'),\n",
              " ('omissions', 'NOUN'),\n",
              " ('in', 'ADP'),\n",
              " ('the', 'DET'),\n",
              " ('Hungarian', 'ADJ'),\n",
              " ('report', 'NOUN'),\n",
              " (\"''\", '.'),\n",
              " ('?', '.'),\n",
              " ('?', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "outputId": "d69fb351-7086-4695-df6a-38329e984896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((34, 4), (34, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTchznarerGz",
        "colab_type": "code",
        "outputId": "3009b7eb-7e45-4c30-fa2b-89363e0878b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_batch[0][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15288.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgWSLxI7Y60q",
        "colab_type": "code",
        "outputId": "6d182ac4-e770-413b-9819-6c185de85480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(tag2ind)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "# got from https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
        "\n",
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()    \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        linear_res = self.hidden2tag(lstm_out)\n",
        "       # tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "       # tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return linear_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtkS9hziHmy-",
        "colab_type": "code",
        "outputId": "e71ec5c1-2a6a-48ec-9e9e-4f574c9880b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(X_batch[0],y_batch[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([15288., 10492., 41245., 23205.]), array([10.,  3.,  8.,  4.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzkYBmR3ixJr",
        "colab_type": "code",
        "outputId": "18a73f6a-a328-4c8c-9d8a-3f77a6789470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "y_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[10.,  3.,  8.,  4.],\n",
              "       [ 3.,  3.,  7.,  3.],\n",
              "       [ 7., 11.,  3.,  8.],\n",
              "       [ 4.,  2.,  5.,  6.],\n",
              "       [ 3.,  8.,  7., 11.],\n",
              "       [11.,  3.,  3.,  4.],\n",
              "       [ 4., 10., 11.,  3.],\n",
              "       [ 3., 10.,  3.,  6.],\n",
              "       [ 7.,  7.,  8., 12.],\n",
              "       [ 3.,  4.,  4.,  8.],\n",
              "       [11.,  8.,  3., 11.],\n",
              "       [ 2.,  3.,  7., 12.],\n",
              "       [ 8., 11.,  4.,  1.],\n",
              "       [11.,  8.,  6.,  8.],\n",
              "       [ 0.,  9.,  3.,  5.],\n",
              "       [ 0.,  5., 11.,  8.],\n",
              "       [ 0.,  6.,  4.,  1.],\n",
              "       [ 0.,  7.,  3.,  8.],\n",
              "       [ 0.,  4.,  7.,  6.],\n",
              "       [ 0.,  3.,  4.,  3.],\n",
              "       [ 0.,  7.,  4.,  7.],\n",
              "       [ 0.,  3.,  3.,  2.],\n",
              "       [ 0., 11.,  8., 11.],\n",
              "       [ 0.,  0.,  8.,  0.],\n",
              "       [ 0.,  0.,  8.,  0.],\n",
              "       [ 0.,  0.,  8.,  0.],\n",
              "       [ 0.,  0.,  2.,  0.],\n",
              "       [ 0.,  0.,  5.,  0.],\n",
              "       [ 0.,  0.,  4.,  0.],\n",
              "       [ 0.,  0.,  6.,  0.],\n",
              "       [ 0.,  0.,  3.,  0.],\n",
              "       [ 0.,  0., 11.,  0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uam8SHCudxzW",
        "colab_type": "code",
        "outputId": "39abb6ba-e595-46e7-9e7e-914fdeb16907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "mask = (torch.LongTensor(y_batch)!=0).float()\n",
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 1., 1., 1.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "outputId": "93411bce-2a51-46f5-9bee-7992ea1b5c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "print('logits shape', logits.shape)\n",
        "indices = torch.argmax(logits,-1)\n",
        "pred = (indices==y_batch).float()\n",
        "print(\"pred shape\", pred.shape)\n",
        "mask = (torch.LongTensor(y_batch)!=0).float() \n",
        "correct_samples = torch.sum(pred*mask)\n",
        "print('correct_samples shape', correct_samples.shape)\n",
        "all_samples = torch.sum(mask)\n",
        "accuracy = correct_samples/all_samples\n",
        "print(accuracy)\n",
        "\n",
        "def get_accuracy(logits, y_batch):\n",
        "    labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)\n",
        "\n",
        "    accuracy = float(torch.sum(labels == y_batch)) / labels.numel()\n",
        "    return accuracy\n",
        "\n",
        "#<calc accuracy>\n",
        "#accuracy = get_accuracy(logits, y_batch)\n",
        "#accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logits shape torch.Size([34, 4, 13])\n",
            "pred shape torch.Size([34, 4])\n",
            "correct_samples shape torch.Size([])\n",
            "tensor(0.0746)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJGrBVscxzp1",
        "colab_type": "code",
        "outputId": "da9fd686-0a7a-4412-98ec-f045d7e5cc5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "tag2ind"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 11,\n",
              " '<pad>': 0,\n",
              " 'ADJ': 6,\n",
              " 'ADP': 7,\n",
              " 'ADV': 5,\n",
              " 'CONJ': 12,\n",
              " 'DET': 4,\n",
              " 'NOUN': 3,\n",
              " 'NUM': 9,\n",
              " 'PRON': 2,\n",
              " 'PRT': 1,\n",
              " 'VERB': 8,\n",
              " 'X': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOS4Ape4ygZ_",
        "colab_type": "code",
        "outputId": "804c57de-4288-4fb8-d464-39e58686bb62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "logits.transpose(2, 1).shape, y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([34, 13, 4]), torch.Size([34, 4]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq8_LXFyJKKF",
        "colab_type": "code",
        "outputId": "9417a3ac-760e-4e5d-bbb9-08950591c5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(logits)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "outputId": "17fe49cd-004a-4143-af7f-65879888db23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = float(criterion(logits.transpose(2,1), y_batch))\n",
        "loss"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.646315574645996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgZKfOsnvwI5",
        "colab_type": "code",
        "outputId": "c4b3a714-eab9-4e15-ce1f-60f208d39370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "logits.transpose(2,1).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 13, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e0Ka025v5fS",
        "colab_type": "code",
        "outputId": "c43791d5-e604-457f-88be-bc516367707e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.view(-1, logits.shape[-1]), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                mask = (y_batch != 0).float()\n",
        "                cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "                cur_sum_count = mask.sum().item()\n",
        "                \n",
        "                # cur_correct_count, cur_sum_count = <calc accuracy>\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "\n",
        "\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "outputId": "72587866-835b-4024-a8a4-3cca691a8ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.30635, Accuracy = 72.16%: 100%|██████████| 572/572 [00:06<00:00, 91.42it/s]\n",
            "[1 / 50]   Val: Loss = 0.10136, Accuracy = 85.34%: 100%|██████████| 13/13 [00:00<00:00, 79.60it/s]\n",
            "[2 / 50] Train: Loss = 0.10004, Accuracy = 90.00%: 100%|██████████| 572/572 [00:06<00:00, 93.88it/s]\n",
            "[2 / 50]   Val: Loss = 0.07033, Accuracy = 89.81%: 100%|██████████| 13/13 [00:00<00:00, 76.89it/s]\n",
            "[3 / 50] Train: Loss = 0.06734, Accuracy = 93.23%: 100%|██████████| 572/572 [00:06<00:00, 94.10it/s]\n",
            "[3 / 50]   Val: Loss = 0.06319, Accuracy = 91.39%: 100%|██████████| 13/13 [00:00<00:00, 75.01it/s]\n",
            "[4 / 50] Train: Loss = 0.05041, Accuracy = 94.84%: 100%|██████████| 572/572 [00:06<00:00, 93.91it/s]\n",
            "[4 / 50]   Val: Loss = 0.05974, Accuracy = 92.24%: 100%|██████████| 13/13 [00:00<00:00, 77.95it/s]\n",
            "[5 / 50] Train: Loss = 0.04051, Accuracy = 95.85%: 100%|██████████| 572/572 [00:06<00:00, 93.81it/s]\n",
            "[5 / 50]   Val: Loss = 0.06239, Accuracy = 92.74%: 100%|██████████| 13/13 [00:00<00:00, 79.28it/s]\n",
            "[6 / 50] Train: Loss = 0.03292, Accuracy = 96.59%: 100%|██████████| 572/572 [00:06<00:00, 93.02it/s]\n",
            "[6 / 50]   Val: Loss = 0.05933, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 72.90it/s]\n",
            "[7 / 50] Train: Loss = 0.02727, Accuracy = 97.17%: 100%|██████████| 572/572 [00:06<00:00, 93.88it/s]\n",
            "[7 / 50]   Val: Loss = 0.05975, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 74.89it/s]\n",
            "[8 / 50] Train: Loss = 0.02247, Accuracy = 97.66%: 100%|██████████| 572/572 [00:06<00:00, 93.45it/s]\n",
            "[8 / 50]   Val: Loss = 0.06083, Accuracy = 93.38%: 100%|██████████| 13/13 [00:00<00:00, 72.90it/s]\n",
            "[9 / 50] Train: Loss = 0.01865, Accuracy = 98.05%: 100%|██████████| 572/572 [00:06<00:00, 93.86it/s]\n",
            "[9 / 50]   Val: Loss = 0.06755, Accuracy = 93.40%: 100%|██████████| 13/13 [00:00<00:00, 78.15it/s]\n",
            "[10 / 50] Train: Loss = 0.01558, Accuracy = 98.39%: 100%|██████████| 572/572 [00:06<00:00, 93.93it/s]\n",
            "[10 / 50]   Val: Loss = 0.07038, Accuracy = 93.44%: 100%|██████████| 13/13 [00:00<00:00, 80.45it/s]\n",
            "[11 / 50] Train: Loss = 0.01279, Accuracy = 98.69%: 100%|██████████| 572/572 [00:06<00:00, 94.47it/s]\n",
            "[11 / 50]   Val: Loss = 0.06585, Accuracy = 93.33%: 100%|██████████| 13/13 [00:00<00:00, 77.02it/s]\n",
            "[12 / 50] Train: Loss = 0.01067, Accuracy = 98.92%: 100%|██████████| 572/572 [00:06<00:00, 94.54it/s]\n",
            "[12 / 50]   Val: Loss = 0.07408, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 77.16it/s]\n",
            "[13 / 50] Train: Loss = 0.00863, Accuracy = 99.12%: 100%|██████████| 572/572 [00:06<00:00, 94.88it/s]\n",
            "[13 / 50]   Val: Loss = 0.07173, Accuracy = 93.25%: 100%|██████████| 13/13 [00:00<00:00, 75.59it/s]\n",
            "[14 / 50] Train: Loss = 0.00708, Accuracy = 99.31%: 100%|██████████| 572/572 [00:06<00:00, 94.64it/s]\n",
            "[14 / 50]   Val: Loss = 0.07419, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 74.48it/s]\n",
            "[15 / 50] Train: Loss = 0.00584, Accuracy = 99.44%: 100%|██████████| 572/572 [00:06<00:00, 94.15it/s]\n",
            "[15 / 50]   Val: Loss = 0.08297, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 80.29it/s]\n",
            "[16 / 50] Train: Loss = 0.00458, Accuracy = 99.58%: 100%|██████████| 572/572 [00:06<00:00, 94.85it/s]\n",
            "[16 / 50]   Val: Loss = 0.08327, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 72.41it/s]\n",
            "[17 / 50] Train: Loss = 0.00383, Accuracy = 99.65%: 100%|██████████| 572/572 [00:06<00:00, 94.93it/s]\n",
            "[17 / 50]   Val: Loss = 0.08856, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 67.76it/s]\n",
            "[18 / 50] Train: Loss = 0.00311, Accuracy = 99.72%: 100%|██████████| 572/572 [00:06<00:00, 94.82it/s]\n",
            "[18 / 50]   Val: Loss = 0.10104, Accuracy = 93.13%: 100%|██████████| 13/13 [00:00<00:00, 78.13it/s]\n",
            "[19 / 50] Train: Loss = 0.00267, Accuracy = 99.76%: 100%|██████████| 572/572 [00:06<00:00, 94.88it/s]\n",
            "[19 / 50]   Val: Loss = 0.09816, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 78.72it/s]\n",
            "[20 / 50] Train: Loss = 0.00239, Accuracy = 99.78%: 100%|██████████| 572/572 [00:06<00:00, 94.47it/s]\n",
            "[20 / 50]   Val: Loss = 0.10454, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 77.38it/s]\n",
            "[21 / 50] Train: Loss = 0.00228, Accuracy = 99.78%: 100%|██████████| 572/572 [00:06<00:00, 94.32it/s]\n",
            "[21 / 50]   Val: Loss = 0.10199, Accuracy = 93.02%: 100%|██████████| 13/13 [00:00<00:00, 74.65it/s]\n",
            "[22 / 50] Train: Loss = 0.00210, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 95.25it/s]\n",
            "[22 / 50]   Val: Loss = 0.11444, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 77.75it/s]\n",
            "[23 / 50] Train: Loss = 0.00196, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 95.91it/s]\n",
            "[23 / 50]   Val: Loss = 0.11086, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 78.90it/s]\n",
            "[24 / 50] Train: Loss = 0.00180, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 95.34it/s]\n",
            "[24 / 50]   Val: Loss = 0.12370, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 80.77it/s]\n",
            "[25 / 50] Train: Loss = 0.00187, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 95.11it/s]\n",
            "[25 / 50]   Val: Loss = 0.11310, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 77.82it/s]\n",
            "[26 / 50] Train: Loss = 0.00188, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 95.11it/s]\n",
            "[26 / 50]   Val: Loss = 0.11819, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 77.13it/s]\n",
            "[27 / 50] Train: Loss = 0.00168, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 95.28it/s]\n",
            "[27 / 50]   Val: Loss = 0.12307, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 80.90it/s]\n",
            "[28 / 50] Train: Loss = 0.00152, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 96.49it/s]\n",
            "[28 / 50]   Val: Loss = 0.11857, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 77.99it/s]\n",
            "[29 / 50] Train: Loss = 0.00145, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.43it/s]\n",
            "[29 / 50]   Val: Loss = 0.12837, Accuracy = 93.14%: 100%|██████████| 13/13 [00:00<00:00, 79.52it/s]\n",
            "[30 / 50] Train: Loss = 0.00151, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.91it/s]\n",
            "[30 / 50]   Val: Loss = 0.13458, Accuracy = 92.90%: 100%|██████████| 13/13 [00:00<00:00, 76.70it/s]\n",
            "[31 / 50] Train: Loss = 0.00233, Accuracy = 99.74%: 100%|██████████| 572/572 [00:06<00:00, 95.03it/s]\n",
            "[31 / 50]   Val: Loss = 0.13174, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 76.57it/s]\n",
            "[32 / 50] Train: Loss = 0.00161, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 94.91it/s]\n",
            "[32 / 50]   Val: Loss = 0.12724, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 76.64it/s]\n",
            "[33 / 50] Train: Loss = 0.00137, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.03it/s]\n",
            "[33 / 50]   Val: Loss = 0.13500, Accuracy = 93.13%: 100%|██████████| 13/13 [00:00<00:00, 77.95it/s]\n",
            "[34 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.38it/s]\n",
            "[34 / 50]   Val: Loss = 0.14315, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 77.90it/s]\n",
            "[35 / 50] Train: Loss = 0.00144, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.89it/s]\n",
            "[35 / 50]   Val: Loss = 0.13081, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 76.46it/s]\n",
            "[36 / 50] Train: Loss = 0.00176, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 94.53it/s]\n",
            "[36 / 50]   Val: Loss = 0.14641, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 75.85it/s]\n",
            "[37 / 50] Train: Loss = 0.00162, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 93.77it/s]\n",
            "[37 / 50]   Val: Loss = 0.15040, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 76.43it/s]\n",
            "[38 / 50] Train: Loss = 0.00143, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 95.34it/s]\n",
            "[38 / 50]   Val: Loss = 0.13621, Accuracy = 93.05%: 100%|██████████| 13/13 [00:00<00:00, 67.29it/s]\n",
            "[39 / 50] Train: Loss = 0.00136, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.49it/s]\n",
            "[39 / 50]   Val: Loss = 0.16308, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 78.09it/s]\n",
            "[40 / 50] Train: Loss = 0.00130, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.19it/s]\n",
            "[40 / 50]   Val: Loss = 0.15347, Accuracy = 93.13%: 100%|██████████| 13/13 [00:00<00:00, 73.28it/s]\n",
            "[41 / 50] Train: Loss = 0.00129, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 94.58it/s]\n",
            "[41 / 50]   Val: Loss = 0.15052, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 75.10it/s]\n",
            "[42 / 50] Train: Loss = 0.00133, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.18it/s]\n",
            "[42 / 50]   Val: Loss = 0.16833, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 77.76it/s]\n",
            "[43 / 50] Train: Loss = 0.00172, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 95.51it/s]\n",
            "[43 / 50]   Val: Loss = 0.15499, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 80.95it/s]\n",
            "[44 / 50] Train: Loss = 0.00186, Accuracy = 99.79%: 100%|██████████| 572/572 [00:06<00:00, 93.77it/s]\n",
            "[44 / 50]   Val: Loss = 0.15237, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 77.33it/s]\n",
            "[45 / 50] Train: Loss = 0.00132, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 92.72it/s]\n",
            "[45 / 50]   Val: Loss = 0.15591, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 75.78it/s]\n",
            "[46 / 50] Train: Loss = 0.00122, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 91.21it/s]\n",
            "[46 / 50]   Val: Loss = 0.15962, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 78.27it/s]\n",
            "[47 / 50] Train: Loss = 0.00123, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 92.33it/s]\n",
            "[47 / 50]   Val: Loss = 0.15543, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 76.14it/s]\n",
            "[48 / 50] Train: Loss = 0.00123, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 94.66it/s]\n",
            "[48 / 50]   Val: Loss = 0.16250, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 74.85it/s]\n",
            "[49 / 50] Train: Loss = 0.00131, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.96it/s]\n",
            "[49 / 50]   Val: Loss = 0.16347, Accuracy = 93.07%: 100%|██████████| 13/13 [00:00<00:00, 76.83it/s]\n",
            "[50 / 50] Train: Loss = 0.00170, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 94.63it/s]\n",
            "[50 / 50]   Val: Loss = 0.16981, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 76.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYAQByu24kel",
        "colab_type": "code",
        "outputId": "de2f93ff-b4d9-4518-8e3a-7786bd0043bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(X_train), len(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36554, 36554)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "outputId": "6e20e431-f805-4e33-cfb2-d6fb266bd706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "current_count = 0\n",
        "sum_count = 0\n",
        "for i,(X_batch, y_batch) in enumerate(iterate_batches((X_train, y_train), 64)):\n",
        "  X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "  logits = model(X_batch)\n",
        "  indices = torch.argmax(logits,-1)\n",
        "  pred = (indices==y_batch).float()\n",
        "\n",
        "  mask = (y_batch!=0).float() \n",
        "  correct_count = torch.sum(pred*mask)\n",
        "  sum_count = torch.sum(mask)\n",
        "  accuracy = correct_samples/all_samples\n",
        "\n",
        "correct_count/sum_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9961, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZU2Sv9bRhHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()    \n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, bidirectional = True)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim*2, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        linear_res = self.hidden2tag(lstm_out)\n",
        "       # tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "       # tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return linear_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwC2XJvmmiJC",
        "colab_type": "code",
        "outputId": "c975a393-e334-4b8e-aff0-767fa6a63c86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=30,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 30] Train: Loss = 0.31934, Accuracy = 71.05%: 100%|██████████| 572/572 [00:06<00:00, 95.20it/s] \n",
            "[1 / 30]   Val: Loss = 0.09402, Accuracy = 85.55%: 100%|██████████| 13/13 [00:00<00:00, 77.83it/s]\n",
            "[2 / 30] Train: Loss = 0.09914, Accuracy = 89.97%: 100%|██████████| 572/572 [00:06<00:00, 95.20it/s]\n",
            "[2 / 30]   Val: Loss = 0.07203, Accuracy = 89.58%: 100%|██████████| 13/13 [00:00<00:00, 79.52it/s]\n",
            "[3 / 30] Train: Loss = 0.06738, Accuracy = 93.18%: 100%|██████████| 572/572 [00:05<00:00, 95.46it/s]\n",
            "[3 / 30]   Val: Loss = 0.06304, Accuracy = 91.36%: 100%|██████████| 13/13 [00:00<00:00, 77.81it/s]\n",
            "[4 / 30] Train: Loss = 0.05070, Accuracy = 94.78%: 100%|██████████| 572/572 [00:06<00:00, 95.01it/s]\n",
            "[4 / 30]   Val: Loss = 0.05728, Accuracy = 92.19%: 100%|██████████| 13/13 [00:00<00:00, 76.24it/s]\n",
            "[5 / 30] Train: Loss = 0.04054, Accuracy = 95.81%: 100%|██████████| 572/572 [00:06<00:00, 94.75it/s]\n",
            "[5 / 30]   Val: Loss = 0.05602, Accuracy = 92.67%: 100%|██████████| 13/13 [00:00<00:00, 73.28it/s]\n",
            "[6 / 30] Train: Loss = 0.03345, Accuracy = 96.55%: 100%|██████████| 572/572 [00:06<00:00, 95.03it/s]\n",
            "[6 / 30]   Val: Loss = 0.05763, Accuracy = 93.06%: 100%|██████████| 13/13 [00:00<00:00, 74.82it/s]\n",
            "[7 / 30] Train: Loss = 0.02759, Accuracy = 97.14%: 100%|██████████| 572/572 [00:06<00:00, 94.93it/s]\n",
            "[7 / 30]   Val: Loss = 0.06207, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 72.27it/s]\n",
            "[8 / 30] Train: Loss = 0.02289, Accuracy = 97.62%: 100%|██████████| 572/572 [00:06<00:00, 89.94it/s]\n",
            "[8 / 30]   Val: Loss = 0.06182, Accuracy = 93.33%: 100%|██████████| 13/13 [00:00<00:00, 76.79it/s]\n",
            "[9 / 30] Train: Loss = 0.01901, Accuracy = 98.01%: 100%|██████████| 572/572 [00:06<00:00, 94.21it/s]\n",
            "[9 / 30]   Val: Loss = 0.06738, Accuracy = 93.37%: 100%|██████████| 13/13 [00:00<00:00, 77.28it/s]\n",
            "[10 / 30] Train: Loss = 0.01604, Accuracy = 98.34%: 100%|██████████| 572/572 [00:06<00:00, 93.81it/s]\n",
            "[10 / 30]   Val: Loss = 0.06816, Accuracy = 93.36%: 100%|██████████| 13/13 [00:00<00:00, 77.59it/s]\n",
            "[11 / 30] Train: Loss = 0.01316, Accuracy = 98.62%: 100%|██████████| 572/572 [00:06<00:00, 94.87it/s]\n",
            "[11 / 30]   Val: Loss = 0.07140, Accuracy = 93.46%: 100%|██████████| 13/13 [00:00<00:00, 78.17it/s]\n",
            "[12 / 30] Train: Loss = 0.01111, Accuracy = 98.87%: 100%|██████████| 572/572 [00:05<00:00, 95.68it/s]\n",
            "[12 / 30]   Val: Loss = 0.07235, Accuracy = 93.37%: 100%|██████████| 13/13 [00:00<00:00, 76.92it/s]\n",
            "[13 / 30] Train: Loss = 0.00909, Accuracy = 99.09%: 100%|██████████| 572/572 [00:06<00:00, 94.60it/s]\n",
            "[13 / 30]   Val: Loss = 0.07692, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 75.40it/s]\n",
            "[14 / 30] Train: Loss = 0.00750, Accuracy = 99.27%: 100%|██████████| 572/572 [00:06<00:00, 94.36it/s]\n",
            "[14 / 30]   Val: Loss = 0.07572, Accuracy = 93.35%: 100%|██████████| 13/13 [00:00<00:00, 77.10it/s]\n",
            "[15 / 30] Train: Loss = 0.00602, Accuracy = 99.43%: 100%|██████████| 572/572 [00:06<00:00, 94.28it/s]\n",
            "[15 / 30]   Val: Loss = 0.08550, Accuracy = 93.35%: 100%|██████████| 13/13 [00:00<00:00, 77.29it/s]\n",
            "[16 / 30] Train: Loss = 0.00492, Accuracy = 99.54%: 100%|██████████| 572/572 [00:06<00:00, 94.83it/s]\n",
            "[16 / 30]   Val: Loss = 0.08755, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 78.68it/s]\n",
            "[17 / 30] Train: Loss = 0.00401, Accuracy = 99.63%: 100%|██████████| 572/572 [00:06<00:00, 95.07it/s]\n",
            "[17 / 30]   Val: Loss = 0.09014, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 78.30it/s]\n",
            "[18 / 30] Train: Loss = 0.00326, Accuracy = 99.70%: 100%|██████████| 572/572 [00:06<00:00, 95.16it/s]\n",
            "[18 / 30]   Val: Loss = 0.08515, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 74.98it/s]\n",
            "[19 / 30] Train: Loss = 0.00278, Accuracy = 99.76%: 100%|██████████| 572/572 [00:05<00:00, 95.46it/s]\n",
            "[19 / 30]   Val: Loss = 0.09637, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 77.50it/s]\n",
            "[20 / 30] Train: Loss = 0.00248, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 95.39it/s]\n",
            "[20 / 30]   Val: Loss = 0.10231, Accuracy = 93.27%: 100%|██████████| 13/13 [00:00<00:00, 77.58it/s]\n",
            "[21 / 30] Train: Loss = 0.00230, Accuracy = 99.79%: 100%|██████████| 572/572 [00:05<00:00, 95.72it/s]\n",
            "[21 / 30]   Val: Loss = 0.10218, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 78.98it/s]\n",
            "[22 / 30] Train: Loss = 0.00228, Accuracy = 99.78%: 100%|██████████| 572/572 [00:06<00:00, 94.09it/s]\n",
            "[22 / 30]   Val: Loss = 0.10846, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 75.80it/s]\n",
            "[23 / 30] Train: Loss = 0.00218, Accuracy = 99.79%: 100%|██████████| 572/572 [00:06<00:00, 94.79it/s]\n",
            "[23 / 30]   Val: Loss = 0.10759, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 76.52it/s]\n",
            "[24 / 30] Train: Loss = 0.00203, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 94.21it/s]\n",
            "[24 / 30]   Val: Loss = 0.11332, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 77.75it/s]\n",
            "[25 / 30] Train: Loss = 0.00171, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 93.80it/s]\n",
            "[25 / 30]   Val: Loss = 0.11960, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 75.76it/s]\n",
            "[26 / 30] Train: Loss = 0.00161, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 93.83it/s]\n",
            "[26 / 30]   Val: Loss = 0.12143, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 76.33it/s]\n",
            "[27 / 30] Train: Loss = 0.00164, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 93.70it/s]\n",
            "[27 / 30]   Val: Loss = 0.11932, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 79.63it/s]\n",
            "[28 / 30] Train: Loss = 0.00170, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 94.38it/s]\n",
            "[28 / 30]   Val: Loss = 0.12786, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 80.86it/s]\n",
            "[29 / 30] Train: Loss = 0.00192, Accuracy = 99.79%: 100%|██████████| 572/572 [00:06<00:00, 94.98it/s]\n",
            "[29 / 30]   Val: Loss = 0.13334, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 80.00it/s]\n",
            "[30 / 30] Train: Loss = 0.00186, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 94.99it/s]\n",
            "[30 / 30]   Val: Loss = 0.13012, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 77.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "outputId": "4d555076-6dc7-43b2-f314-a30ed1f09516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:410: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "outputId": "57506275-1c43-4caa-fb5d-b636fcb79d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMn0MXyfnpEH",
        "colab_type": "code",
        "outputId": "0a5e3a41-45fa-4493-98fb-107067fd774a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2v_model.vectors.shape[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers = lstm_layers_count)\n",
        "        \n",
        "        self.hidden2tag = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.word_embeddings(inputs)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        linear_res = self.hidden2tag(lstm_out)\n",
        "       # tag_space = self.hidden2tag(lstm_out.view(inputs.numel(), -1))\n",
        "       # tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return linear_res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "outputId": "270f3274-2971-4132-a5dc-62efeb97bb03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.75088, Accuracy = 77.97%: 100%|██████████| 572/572 [00:04<00:00, 129.06it/s]\n",
            "[1 / 50]   Val: Loss = 0.37344, Accuracy = 89.15%: 100%|██████████| 13/13 [00:00<00:00, 88.28it/s]\n",
            "[2 / 50] Train: Loss = 0.28881, Accuracy = 91.34%: 100%|██████████| 572/572 [00:04<00:00, 130.63it/s]\n",
            "[2 / 50]   Val: Loss = 0.26206, Accuracy = 91.98%: 100%|██████████| 13/13 [00:00<00:00, 77.59it/s]\n",
            "[3 / 50] Train: Loss = 0.21246, Accuracy = 93.38%: 100%|██████████| 572/572 [00:04<00:00, 123.21it/s]\n",
            "[3 / 50]   Val: Loss = 0.21389, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 84.69it/s]\n",
            "[4 / 50] Train: Loss = 0.17566, Accuracy = 94.42%: 100%|██████████| 572/572 [00:04<00:00, 131.86it/s]\n",
            "[4 / 50]   Val: Loss = 0.18707, Accuracy = 94.05%: 100%|██████████| 13/13 [00:00<00:00, 85.94it/s]\n",
            "[5 / 50] Train: Loss = 0.15357, Accuracy = 95.05%: 100%|██████████| 572/572 [00:04<00:00, 132.25it/s]\n",
            "[5 / 50]   Val: Loss = 0.17166, Accuracy = 94.45%: 100%|██████████| 13/13 [00:00<00:00, 84.56it/s]\n",
            "[6 / 50] Train: Loss = 0.13932, Accuracy = 95.43%: 100%|██████████| 572/572 [00:04<00:00, 132.58it/s]\n",
            "[6 / 50]   Val: Loss = 0.16062, Accuracy = 94.73%: 100%|██████████| 13/13 [00:00<00:00, 85.57it/s]\n",
            "[7 / 50] Train: Loss = 0.12918, Accuracy = 95.71%: 100%|██████████| 572/572 [00:04<00:00, 132.95it/s]\n",
            "[7 / 50]   Val: Loss = 0.15372, Accuracy = 94.89%: 100%|██████████| 13/13 [00:00<00:00, 91.79it/s]\n",
            "[8 / 50] Train: Loss = 0.12146, Accuracy = 95.93%: 100%|██████████| 572/572 [00:04<00:00, 130.58it/s]\n",
            "[8 / 50]   Val: Loss = 0.14933, Accuracy = 95.08%: 100%|██████████| 13/13 [00:00<00:00, 88.86it/s]\n",
            "[9 / 50] Train: Loss = 0.11567, Accuracy = 96.11%: 100%|██████████| 572/572 [00:04<00:00, 131.74it/s]\n",
            "[9 / 50]   Val: Loss = 0.14540, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 87.67it/s]\n",
            "[10 / 50] Train: Loss = 0.11072, Accuracy = 96.24%: 100%|██████████| 572/572 [00:04<00:00, 131.54it/s]\n",
            "[10 / 50]   Val: Loss = 0.14298, Accuracy = 95.17%: 100%|██████████| 13/13 [00:00<00:00, 88.01it/s]\n",
            "[11 / 50] Train: Loss = 0.10690, Accuracy = 96.35%: 100%|██████████| 572/572 [00:04<00:00, 131.38it/s]\n",
            "[11 / 50]   Val: Loss = 0.14049, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 84.44it/s]\n",
            "[12 / 50] Train: Loss = 0.10352, Accuracy = 96.44%: 100%|██████████| 572/572 [00:04<00:00, 126.31it/s]\n",
            "[12 / 50]   Val: Loss = 0.13855, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 83.27it/s]\n",
            "[13 / 50] Train: Loss = 0.10057, Accuracy = 96.50%: 100%|██████████| 572/572 [00:04<00:00, 126.53it/s]\n",
            "[13 / 50]   Val: Loss = 0.13847, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 89.50it/s]\n",
            "[14 / 50] Train: Loss = 0.09793, Accuracy = 96.60%: 100%|██████████| 572/572 [00:04<00:00, 132.22it/s]\n",
            "[14 / 50]   Val: Loss = 0.13670, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 87.75it/s]\n",
            "[15 / 50] Train: Loss = 0.09562, Accuracy = 96.66%: 100%|██████████| 572/572 [00:04<00:00, 130.39it/s]\n",
            "[15 / 50]   Val: Loss = 0.13555, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 85.35it/s]\n",
            "[16 / 50] Train: Loss = 0.09361, Accuracy = 96.73%: 100%|██████████| 572/572 [00:04<00:00, 130.63it/s]\n",
            "[16 / 50]   Val: Loss = 0.13627, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 85.54it/s]\n",
            "[17 / 50] Train: Loss = 0.09198, Accuracy = 96.77%: 100%|██████████| 572/572 [00:04<00:00, 133.10it/s]\n",
            "[17 / 50]   Val: Loss = 0.13448, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 87.55it/s]\n",
            "[18 / 50] Train: Loss = 0.08994, Accuracy = 96.82%: 100%|██████████| 572/572 [00:04<00:00, 133.98it/s]\n",
            "[18 / 50]   Val: Loss = 0.13623, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 89.62it/s]\n",
            "[19 / 50] Train: Loss = 0.08847, Accuracy = 96.87%: 100%|██████████| 572/572 [00:04<00:00, 133.17it/s]\n",
            "[19 / 50]   Val: Loss = 0.13245, Accuracy = 95.40%: 100%|██████████| 13/13 [00:00<00:00, 90.49it/s]\n",
            "[20 / 50] Train: Loss = 0.08689, Accuracy = 96.92%: 100%|██████████| 572/572 [00:04<00:00, 133.23it/s]\n",
            "[20 / 50]   Val: Loss = 0.13515, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 88.42it/s]\n",
            "[21 / 50] Train: Loss = 0.08545, Accuracy = 96.97%: 100%|██████████| 572/572 [00:04<00:00, 132.24it/s]\n",
            "[21 / 50]   Val: Loss = 0.13361, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 82.47it/s]\n",
            "[22 / 50] Train: Loss = 0.08433, Accuracy = 97.02%: 100%|██████████| 572/572 [00:04<00:00, 132.57it/s]\n",
            "[22 / 50]   Val: Loss = 0.13419, Accuracy = 95.52%: 100%|██████████| 13/13 [00:00<00:00, 89.80it/s]\n",
            "[23 / 50] Train: Loss = 0.08301, Accuracy = 97.06%: 100%|██████████| 572/572 [00:04<00:00, 131.68it/s]\n",
            "[23 / 50]   Val: Loss = 0.13236, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 91.48it/s]\n",
            "[24 / 50] Train: Loss = 0.08175, Accuracy = 97.09%: 100%|██████████| 572/572 [00:04<00:00, 133.99it/s]\n",
            "[24 / 50]   Val: Loss = 0.13232, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 87.31it/s]\n",
            "[25 / 50] Train: Loss = 0.08055, Accuracy = 97.16%: 100%|██████████| 572/572 [00:04<00:00, 135.08it/s]\n",
            "[25 / 50]   Val: Loss = 0.13273, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 87.54it/s]\n",
            "[26 / 50] Train: Loss = 0.07956, Accuracy = 97.17%: 100%|██████████| 572/572 [00:04<00:00, 133.97it/s]\n",
            "[26 / 50]   Val: Loss = 0.13363, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 90.10it/s]\n",
            "[27 / 50] Train: Loss = 0.07842, Accuracy = 97.21%: 100%|██████████| 572/572 [00:04<00:00, 132.69it/s]\n",
            "[27 / 50]   Val: Loss = 0.13351, Accuracy = 95.54%: 100%|██████████| 13/13 [00:00<00:00, 88.64it/s]\n",
            "[28 / 50] Train: Loss = 0.07763, Accuracy = 97.23%: 100%|██████████| 572/572 [00:04<00:00, 132.06it/s]\n",
            "[28 / 50]   Val: Loss = 0.13417, Accuracy = 95.42%: 100%|██████████| 13/13 [00:00<00:00, 93.93it/s]\n",
            "[29 / 50] Train: Loss = 0.07670, Accuracy = 97.25%: 100%|██████████| 572/572 [00:04<00:00, 134.28it/s]\n",
            "[29 / 50]   Val: Loss = 0.13320, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 89.72it/s]\n",
            "[30 / 50] Train: Loss = 0.07568, Accuracy = 97.29%: 100%|██████████| 572/572 [00:04<00:00, 131.34it/s]\n",
            "[30 / 50]   Val: Loss = 0.13376, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 92.87it/s]\n",
            "[31 / 50] Train: Loss = 0.07487, Accuracy = 97.31%: 100%|██████████| 572/572 [00:04<00:00, 135.44it/s]\n",
            "[31 / 50]   Val: Loss = 0.13501, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 89.51it/s]\n",
            "[32 / 50] Train: Loss = 0.07396, Accuracy = 97.34%: 100%|██████████| 572/572 [00:04<00:00, 134.80it/s]\n",
            "[32 / 50]   Val: Loss = 0.13516, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 87.77it/s]\n",
            "[33 / 50] Train: Loss = 0.07316, Accuracy = 97.37%: 100%|██████████| 572/572 [00:04<00:00, 132.97it/s]\n",
            "[33 / 50]   Val: Loss = 0.13567, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 85.59it/s]\n",
            "[34 / 50] Train: Loss = 0.07246, Accuracy = 97.40%: 100%|██████████| 572/572 [00:04<00:00, 132.72it/s]\n",
            "[34 / 50]   Val: Loss = 0.13558, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 86.90it/s]\n",
            "[35 / 50] Train: Loss = 0.07179, Accuracy = 97.43%: 100%|██████████| 572/572 [00:04<00:00, 133.19it/s]\n",
            "[35 / 50]   Val: Loss = 0.13833, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 88.66it/s]\n",
            "[36 / 50] Train: Loss = 0.07109, Accuracy = 97.45%: 100%|██████████| 572/572 [00:04<00:00, 132.63it/s]\n",
            "[36 / 50]   Val: Loss = 0.13760, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 93.53it/s]\n",
            "[37 / 50] Train: Loss = 0.07037, Accuracy = 97.49%: 100%|██████████| 572/572 [00:04<00:00, 132.54it/s]\n",
            "[37 / 50]   Val: Loss = 0.13678, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 89.36it/s]\n",
            "[38 / 50] Train: Loss = 0.06969, Accuracy = 97.52%: 100%|██████████| 572/572 [00:04<00:00, 132.73it/s]\n",
            "[38 / 50]   Val: Loss = 0.13857, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 90.93it/s]\n",
            "[39 / 50] Train: Loss = 0.06891, Accuracy = 97.53%: 100%|██████████| 572/572 [00:04<00:00, 133.38it/s]\n",
            "[39 / 50]   Val: Loss = 0.13819, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 88.45it/s]\n",
            "[40 / 50] Train: Loss = 0.06818, Accuracy = 97.56%: 100%|██████████| 572/572 [00:04<00:00, 132.83it/s]\n",
            "[40 / 50]   Val: Loss = 0.14017, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 89.50it/s]\n",
            "[41 / 50] Train: Loss = 0.06774, Accuracy = 97.58%: 100%|██████████| 572/572 [00:04<00:00, 134.75it/s]\n",
            "[41 / 50]   Val: Loss = 0.14031, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 90.78it/s]\n",
            "[42 / 50] Train: Loss = 0.06719, Accuracy = 97.59%: 100%|██████████| 572/572 [00:04<00:00, 136.16it/s]\n",
            "[42 / 50]   Val: Loss = 0.14159, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 89.03it/s]\n",
            "[43 / 50] Train: Loss = 0.06645, Accuracy = 97.62%: 100%|██████████| 572/572 [00:04<00:00, 133.12it/s]\n",
            "[43 / 50]   Val: Loss = 0.14187, Accuracy = 95.25%: 100%|██████████| 13/13 [00:00<00:00, 88.08it/s]\n",
            "[44 / 50] Train: Loss = 0.06579, Accuracy = 97.65%: 100%|██████████| 572/572 [00:04<00:00, 134.96it/s]\n",
            "[44 / 50]   Val: Loss = 0.14386, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 88.83it/s]\n",
            "[45 / 50] Train: Loss = 0.06525, Accuracy = 97.66%: 100%|██████████| 572/572 [00:04<00:00, 134.40it/s]\n",
            "[45 / 50]   Val: Loss = 0.14283, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 85.47it/s]\n",
            "[46 / 50] Train: Loss = 0.06457, Accuracy = 97.70%: 100%|██████████| 572/572 [00:04<00:00, 136.26it/s]\n",
            "[46 / 50]   Val: Loss = 0.14288, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 89.41it/s]\n",
            "[47 / 50] Train: Loss = 0.06404, Accuracy = 97.71%: 100%|██████████| 572/572 [00:04<00:00, 136.52it/s]\n",
            "[47 / 50]   Val: Loss = 0.14460, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 89.00it/s]\n",
            "[48 / 50] Train: Loss = 0.06348, Accuracy = 97.73%: 100%|██████████| 572/572 [00:04<00:00, 135.84it/s]\n",
            "[48 / 50]   Val: Loss = 0.14554, Accuracy = 95.16%: 100%|██████████| 13/13 [00:00<00:00, 88.17it/s]\n",
            "[49 / 50] Train: Loss = 0.06304, Accuracy = 97.74%: 100%|██████████| 572/572 [00:04<00:00, 134.27it/s]\n",
            "[49 / 50]   Val: Loss = 0.14625, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 87.40it/s]\n",
            "[50 / 50] Train: Loss = 0.06267, Accuracy = 97.76%: 100%|██████████| 572/572 [00:04<00:00, 134.14it/s]\n",
            "[50 / 50]   Val: Loss = 0.14585, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 87.98it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMLst6OOpOI6",
        "colab_type": "code",
        "outputId": "42e143ab-31f2-4dbb-9f24-683ef69d37b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embeddings.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45441, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "outputId": "e78fe098-27bc-4c14-a706-895d7e3f8a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#<calc test accuracy>\n",
        "current_count = 0\n",
        "sum_count = 0\n",
        "for i,(X_batch, y_batch) in enumerate(iterate_batches((X_train, y_train), 64)):\n",
        "  X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "  logits = model(X_batch)\n",
        "  indices = torch.argmax(logits,-1)\n",
        "  pred = (indices==y_batch).float()\n",
        "\n",
        "  mask = (y_batch!=0).float() \n",
        "  correct_count = torch.sum(pred*mask)\n",
        "  sum_count = torch.sum(mask)\n",
        "  accuracy = correct_samples/all_samples\n",
        "\n",
        "correct_count/sum_count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9851, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    }
  ]
}