{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W <layers.Param object at 0xb179f50b8>\n",
      "B <layers.Param object at 0xb179f5e10>\n"
     ]
    }
   ],
   "source": [
    "for item, param in FullyConnectedLayer(3, 4).params().items():\n",
    "    print(item, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for layer1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for layer3.W\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for layer1.W\n",
      "Gradient check passed!\n",
      "Checking gradient for layer1.B\n",
      "Gradient check passed!\n",
      "Checking gradient for layer3.W\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03333333333333333"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 3072)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.222282, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262136, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.161360, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.208817, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.215671, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.375831, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.279087, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296794, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.352776, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.356868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.145697, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.084675, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.096825, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182941, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226713, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.324955, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.255105, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.314281, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.205500, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286170, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1051150f0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb18556cf8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.263324, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.220223, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301698, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.335452, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243683, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.265556, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.318213, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.282019, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302962, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266117, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239290, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.239442, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303512, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.283935, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.249252, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291344, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303446, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292269, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308432, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.267712, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Learning rate shouldn'tve been reduced that much!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2cd0d7e60500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate should've been reduced\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minitial_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Learning rate shouldn'tve been reduced that much!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Learning rate shouldn'tve been reduced that much!"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302360, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296570, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309396, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299183, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306074, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301357, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298459, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291714, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298135, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298849, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296528, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305571, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.307247, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299542, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.285864, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297577, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.295232, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305770, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304844, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292936, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.313330, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.295656, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.267304, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.301773, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.275044, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.247942, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.210456, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.277450, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.185708, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.169285, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.249678, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.951585, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.876698, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.940951, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.121283, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.253735, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.987482, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.908443, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.556664, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.513839, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.589340, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.452519, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.321631, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.438304, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.114777, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.477361, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.359391, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.960485, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: 2.241122, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.127206, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 1.616395, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: 2.028534, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.830794, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.393634, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.489514, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.675337, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.799013, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.712184, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.947728, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.010247, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.677269, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.711555, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.115563, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.473389, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.550967, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.081456, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.673656, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.413661, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.792638, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.667819, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.730563, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.245205, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.234047, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.465152, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 2.052438, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.895880, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.573863, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.346063, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.223363, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.507529, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.329266, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.707389, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.844443, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.528234, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.125265, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.170777, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.390364, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.871876, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.387109, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.512464, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.393327, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.418344, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.766237, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.798037, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.751419, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.118516, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.065695, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.622125, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.372487, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.317716, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.341970, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.699377, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.574757, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.491428, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.747660, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.813047, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.230409, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.867783, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.382730, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.860690, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.384831, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.286017, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.935622, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.949048, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.378486, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.977516, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.277505, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.578128, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.178259, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.218723, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.240859, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.164055, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.412850, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.052664, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.471199, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.243748, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.684508, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.740886, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.973135, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.872325, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.259177, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.181848, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.431496, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.508625, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.306184, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.177009, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.687489, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.908380, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.728621, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.530479, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.741164, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.825606, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.134661, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.544514, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.218836, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.154048, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.572458, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.152982, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.366568, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.233118, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.342978, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.976894, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.119878, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.704564, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 0.802525, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.338506, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.586959, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.049498, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.617902, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.525341, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.088913, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.432742, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.153628, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.497889, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.238333, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.368631, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.149772, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.307122, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.424940, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.445682, Train accuracy: 0.866667, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.306644, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.293270, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.296320, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.253260, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.287459, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.305922, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.313797, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.242170, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.126024, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.119446, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.182628, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.245882, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.108874, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.622275, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.714791, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.782290, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.484125, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.492229, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.088223, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.407535, Train accuracy: 0.466667, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 0)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.291047, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278075, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.269545, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.246676, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237251, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252818, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250834, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.262939, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.243491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.201578, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.252455, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270932, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.271639, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.186965, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.226151, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211028, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.200939, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.173949, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237739, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.254611, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229384, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212213, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.151976, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217323, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.229902, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270075, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.196633, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.273132, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.277519, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.214682, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.219945, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.237401, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.157900, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.186875, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.213531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.275720, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.199232, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.270968, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.195771, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.217145, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209910, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.182411, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.118846, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.209434, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.211120, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.139029, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.170882, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.206762, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.174950, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.198797, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.284913, Train accuracy: 0.200111, val accuracy: 0.210000\n",
      "Loss: 2.166318, Train accuracy: 0.206222, val accuracy: 0.212000\n",
      "Loss: 2.142690, Train accuracy: 0.210333, val accuracy: 0.214000\n",
      "Loss: 2.082000, Train accuracy: 0.213667, val accuracy: 0.218000\n",
      "Loss: 2.114584, Train accuracy: 0.218556, val accuracy: 0.229000\n",
      "Loss: 2.167487, Train accuracy: 0.222778, val accuracy: 0.227000\n",
      "Loss: 2.214893, Train accuracy: 0.227333, val accuracy: 0.236000\n",
      "Loss: 2.113952, Train accuracy: 0.230556, val accuracy: 0.237000\n",
      "Loss: 2.152822, Train accuracy: 0.233889, val accuracy: 0.243000\n",
      "Loss: 2.228245, Train accuracy: 0.237444, val accuracy: 0.243000\n",
      "Loss: 2.136111, Train accuracy: 0.240222, val accuracy: 0.245000\n",
      "Loss: 2.074438, Train accuracy: 0.244556, val accuracy: 0.246000\n",
      "Loss: 2.119400, Train accuracy: 0.245111, val accuracy: 0.244000\n",
      "Loss: 2.127400, Train accuracy: 0.248111, val accuracy: 0.249000\n",
      "Loss: 2.091803, Train accuracy: 0.251000, val accuracy: 0.251000\n",
      "Loss: 2.123375, Train accuracy: 0.253667, val accuracy: 0.250000\n",
      "Loss: 2.102194, Train accuracy: 0.255111, val accuracy: 0.252000\n",
      "Loss: 2.139020, Train accuracy: 0.259667, val accuracy: 0.258000\n",
      "Loss: 2.092506, Train accuracy: 0.257556, val accuracy: 0.255000\n",
      "Loss: 2.053369, Train accuracy: 0.258556, val accuracy: 0.257000\n",
      "Loss: 2.093309, Train accuracy: 0.263222, val accuracy: 0.260000\n",
      "Loss: 2.134876, Train accuracy: 0.260667, val accuracy: 0.261000\n",
      "Loss: 2.152777, Train accuracy: 0.266111, val accuracy: 0.266000\n",
      "Loss: 2.066246, Train accuracy: 0.265889, val accuracy: 0.269000\n",
      "Loss: 2.065951, Train accuracy: 0.268111, val accuracy: 0.265000\n",
      "Loss: 2.069840, Train accuracy: 0.266222, val accuracy: 0.268000\n",
      "Loss: 2.056676, Train accuracy: 0.268333, val accuracy: 0.267000\n",
      "Loss: 2.096115, Train accuracy: 0.268667, val accuracy: 0.268000\n",
      "Loss: 2.062471, Train accuracy: 0.271556, val accuracy: 0.268000\n",
      "Loss: 1.989163, Train accuracy: 0.270556, val accuracy: 0.269000\n",
      "Loss: 2.004363, Train accuracy: 0.271556, val accuracy: 0.269000\n",
      "Loss: 2.079799, Train accuracy: 0.272556, val accuracy: 0.273000\n",
      "Loss: 2.043503, Train accuracy: 0.273222, val accuracy: 0.273000\n",
      "Loss: 2.069092, Train accuracy: 0.273333, val accuracy: 0.275000\n",
      "Loss: 2.008402, Train accuracy: 0.274000, val accuracy: 0.277000\n",
      "Loss: 2.014805, Train accuracy: 0.274222, val accuracy: 0.279000\n",
      "Loss: 2.018551, Train accuracy: 0.275556, val accuracy: 0.277000\n",
      "Loss: 2.112460, Train accuracy: 0.277889, val accuracy: 0.283000\n",
      "Loss: 2.114885, Train accuracy: 0.278000, val accuracy: 0.278000\n",
      "Loss: 1.998552, Train accuracy: 0.279333, val accuracy: 0.277000\n",
      "Loss: 1.968963, Train accuracy: 0.283111, val accuracy: 0.284000\n",
      "Loss: 1.992542, Train accuracy: 0.284444, val accuracy: 0.290000\n",
      "Loss: 1.944149, Train accuracy: 0.285111, val accuracy: 0.290000\n",
      "Loss: 2.017768, Train accuracy: 0.286889, val accuracy: 0.291000\n",
      "Loss: 1.964418, Train accuracy: 0.289000, val accuracy: 0.292000\n",
      "Loss: 1.892692, Train accuracy: 0.293000, val accuracy: 0.299000\n",
      "Loss: 1.931428, Train accuracy: 0.296000, val accuracy: 0.302000\n",
      "Loss: 1.958612, Train accuracy: 0.301333, val accuracy: 0.307000\n",
      "Loss: 1.881982, Train accuracy: 0.306222, val accuracy: 0.314000\n",
      "Loss: 2.105934, Train accuracy: 0.310333, val accuracy: 0.314000\n",
      "Loss: 1.926241, Train accuracy: 0.314444, val accuracy: 0.316000\n",
      "Loss: 1.876246, Train accuracy: 0.319556, val accuracy: 0.321000\n",
      "Loss: 2.140196, Train accuracy: 0.323444, val accuracy: 0.323000\n",
      "Loss: 1.946637, Train accuracy: 0.327889, val accuracy: 0.324000\n",
      "Loss: 1.897765, Train accuracy: 0.329667, val accuracy: 0.328000\n",
      "Loss: 2.053009, Train accuracy: 0.334778, val accuracy: 0.331000\n",
      "Loss: 1.894593, Train accuracy: 0.339444, val accuracy: 0.338000\n",
      "Loss: 1.926854, Train accuracy: 0.342111, val accuracy: 0.342000\n",
      "Loss: 1.942384, Train accuracy: 0.344889, val accuracy: 0.340000\n",
      "Loss: 1.871854, Train accuracy: 0.347889, val accuracy: 0.343000\n",
      "Loss: 1.851447, Train accuracy: 0.349889, val accuracy: 0.344000\n",
      "Loss: 1.973409, Train accuracy: 0.350778, val accuracy: 0.345000\n",
      "Loss: 1.684712, Train accuracy: 0.353778, val accuracy: 0.348000\n",
      "Loss: 1.951727, Train accuracy: 0.357222, val accuracy: 0.352000\n",
      "Loss: 1.814449, Train accuracy: 0.359778, val accuracy: 0.352000\n",
      "Loss: 1.838696, Train accuracy: 0.362778, val accuracy: 0.358000\n",
      "Loss: 1.861580, Train accuracy: 0.365444, val accuracy: 0.360000\n",
      "Loss: 1.818320, Train accuracy: 0.368889, val accuracy: 0.366000\n",
      "Loss: 1.862279, Train accuracy: 0.369333, val accuracy: 0.365000\n",
      "Loss: 1.683575, Train accuracy: 0.372778, val accuracy: 0.367000\n",
      "Loss: 1.934408, Train accuracy: 0.375000, val accuracy: 0.366000\n",
      "Loss: 1.860512, Train accuracy: 0.376889, val accuracy: 0.373000\n",
      "Loss: 1.958449, Train accuracy: 0.379111, val accuracy: 0.364000\n",
      "Loss: 1.895373, Train accuracy: 0.381778, val accuracy: 0.378000\n",
      "Loss: 1.843549, Train accuracy: 0.383778, val accuracy: 0.381000\n",
      "Loss: 1.780220, Train accuracy: 0.385778, val accuracy: 0.381000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.654622, Train accuracy: 0.387778, val accuracy: 0.383000\n",
      "Loss: 1.769559, Train accuracy: 0.389222, val accuracy: 0.390000\n",
      "Loss: 1.641381, Train accuracy: 0.391667, val accuracy: 0.385000\n",
      "Loss: 1.735905, Train accuracy: 0.394556, val accuracy: 0.385000\n",
      "Loss: 1.732951, Train accuracy: 0.396222, val accuracy: 0.386000\n",
      "Loss: 1.805496, Train accuracy: 0.397556, val accuracy: 0.385000\n",
      "Loss: 1.653400, Train accuracy: 0.398222, val accuracy: 0.393000\n",
      "Loss: 1.693471, Train accuracy: 0.400111, val accuracy: 0.388000\n",
      "Loss: 1.504346, Train accuracy: 0.401000, val accuracy: 0.393000\n",
      "Loss: 1.693361, Train accuracy: 0.404556, val accuracy: 0.390000\n",
      "Loss: 1.657454, Train accuracy: 0.407111, val accuracy: 0.397000\n",
      "Loss: 1.685120, Train accuracy: 0.406778, val accuracy: 0.397000\n",
      "Loss: 1.718916, Train accuracy: 0.410444, val accuracy: 0.395000\n",
      "Loss: 1.634462, Train accuracy: 0.411111, val accuracy: 0.403000\n",
      "Loss: 1.788143, Train accuracy: 0.417556, val accuracy: 0.402000\n",
      "Loss: 1.769072, Train accuracy: 0.417111, val accuracy: 0.398000\n",
      "Loss: 1.820411, Train accuracy: 0.420556, val accuracy: 0.401000\n",
      "Loss: 1.730185, Train accuracy: 0.420333, val accuracy: 0.400000\n",
      "Loss: 1.561077, Train accuracy: 0.421667, val accuracy: 0.402000\n",
      "Loss: 1.592264, Train accuracy: 0.422667, val accuracy: 0.404000\n",
      "Loss: 1.827856, Train accuracy: 0.423111, val accuracy: 0.406000\n",
      "Loss: 1.633718, Train accuracy: 0.425000, val accuracy: 0.406000\n",
      "Loss: 1.713303, Train accuracy: 0.426667, val accuracy: 0.408000\n",
      "Loss: 1.649540, Train accuracy: 0.430111, val accuracy: 0.409000\n",
      "Loss: 1.801208, Train accuracy: 0.429667, val accuracy: 0.409000\n",
      "Loss: 1.737050, Train accuracy: 0.432222, val accuracy: 0.411000\n",
      "Loss: 1.756633, Train accuracy: 0.434111, val accuracy: 0.414000\n",
      "Loss: 1.723631, Train accuracy: 0.434222, val accuracy: 0.415000\n",
      "Loss: 1.763978, Train accuracy: 0.437111, val accuracy: 0.413000\n",
      "Loss: 1.567596, Train accuracy: 0.438222, val accuracy: 0.415000\n",
      "Loss: 1.716929, Train accuracy: 0.441333, val accuracy: 0.420000\n",
      "Loss: 1.520596, Train accuracy: 0.443667, val accuracy: 0.422000\n",
      "Loss: 1.643480, Train accuracy: 0.444667, val accuracy: 0.420000\n",
      "Loss: 1.591679, Train accuracy: 0.447889, val accuracy: 0.419000\n",
      "Loss: 1.690682, Train accuracy: 0.450222, val accuracy: 0.431000\n",
      "Loss: 1.670727, Train accuracy: 0.451444, val accuracy: 0.432000\n",
      "Loss: 1.630840, Train accuracy: 0.454111, val accuracy: 0.426000\n",
      "Loss: 1.524444, Train accuracy: 0.456222, val accuracy: 0.429000\n",
      "Loss: 1.533880, Train accuracy: 0.456111, val accuracy: 0.432000\n",
      "Loss: 1.795499, Train accuracy: 0.460111, val accuracy: 0.435000\n",
      "Loss: 1.707028, Train accuracy: 0.464000, val accuracy: 0.437000\n",
      "Loss: 1.622401, Train accuracy: 0.464333, val accuracy: 0.442000\n",
      "Loss: 1.621550, Train accuracy: 0.465889, val accuracy: 0.446000\n",
      "Loss: 1.503821, Train accuracy: 0.468667, val accuracy: 0.449000\n",
      "Loss: 1.641435, Train accuracy: 0.469444, val accuracy: 0.456000\n",
      "Loss: 1.603233, Train accuracy: 0.470444, val accuracy: 0.456000\n",
      "Loss: 1.639929, Train accuracy: 0.471889, val accuracy: 0.450000\n",
      "Loss: 1.562898, Train accuracy: 0.472556, val accuracy: 0.457000\n",
      "Loss: 1.427918, Train accuracy: 0.478778, val accuracy: 0.461000\n",
      "Loss: 1.610222, Train accuracy: 0.478778, val accuracy: 0.462000\n",
      "Loss: 1.585153, Train accuracy: 0.481333, val accuracy: 0.461000\n",
      "Loss: 1.547077, Train accuracy: 0.481778, val accuracy: 0.462000\n",
      "Loss: 1.747207, Train accuracy: 0.484556, val accuracy: 0.468000\n",
      "Loss: 1.646079, Train accuracy: 0.487778, val accuracy: 0.471000\n",
      "Loss: 1.619843, Train accuracy: 0.486556, val accuracy: 0.466000\n",
      "Loss: 1.679447, Train accuracy: 0.489667, val accuracy: 0.471000\n",
      "Loss: 1.604914, Train accuracy: 0.492444, val accuracy: 0.467000\n",
      "Loss: 1.652482, Train accuracy: 0.492333, val accuracy: 0.475000\n",
      "Loss: 1.699402, Train accuracy: 0.494222, val accuracy: 0.474000\n",
      "Loss: 1.515305, Train accuracy: 0.495667, val accuracy: 0.475000\n",
      "Loss: 1.522715, Train accuracy: 0.494778, val accuracy: 0.479000\n",
      "Loss: 1.388927, Train accuracy: 0.501222, val accuracy: 0.482000\n",
      "Loss: 1.569881, Train accuracy: 0.502889, val accuracy: 0.485000\n",
      "Loss: 1.581636, Train accuracy: 0.503000, val accuracy: 0.478000\n",
      "Loss: 1.466344, Train accuracy: 0.504444, val accuracy: 0.487000\n",
      "Loss: 1.530445, Train accuracy: 0.505556, val accuracy: 0.488000\n",
      "Loss: 1.463536, Train accuracy: 0.504889, val accuracy: 0.488000\n",
      "Loss: 1.552092, Train accuracy: 0.508333, val accuracy: 0.490000\n",
      "Loss: 1.571990, Train accuracy: 0.511444, val accuracy: 0.491000\n",
      "Loss: 1.454132, Train accuracy: 0.513111, val accuracy: 0.494000\n",
      "Loss: 1.567457, Train accuracy: 0.512556, val accuracy: 0.493000\n",
      "Loss: 1.373993, Train accuracy: 0.515333, val accuracy: 0.497000\n",
      "Loss: 1.471717, Train accuracy: 0.517111, val accuracy: 0.494000\n",
      "Loss: 1.312817, Train accuracy: 0.515667, val accuracy: 0.494000\n",
      "Loss: 1.548993, Train accuracy: 0.517222, val accuracy: 0.496000\n",
      "Loss: 1.424993, Train accuracy: 0.519889, val accuracy: 0.501000\n",
      "Loss: 1.499554, Train accuracy: 0.522000, val accuracy: 0.503000\n",
      "Loss: 1.451656, Train accuracy: 0.524444, val accuracy: 0.507000\n",
      "Loss: 1.513728, Train accuracy: 0.525889, val accuracy: 0.509000\n",
      "Loss: 1.487762, Train accuracy: 0.527000, val accuracy: 0.506000\n",
      "Loss: 1.554240, Train accuracy: 0.529111, val accuracy: 0.506000\n",
      "Loss: 1.556140, Train accuracy: 0.530111, val accuracy: 0.516000\n",
      "Loss: 1.481277, Train accuracy: 0.530333, val accuracy: 0.507000\n",
      "Loss: 1.660517, Train accuracy: 0.532556, val accuracy: 0.518000\n",
      "Loss: 1.624583, Train accuracy: 0.532556, val accuracy: 0.516000\n",
      "Loss: 1.759726, Train accuracy: 0.534667, val accuracy: 0.516000\n",
      "Loss: 1.564118, Train accuracy: 0.533889, val accuracy: 0.510000\n",
      "Loss: 1.555806, Train accuracy: 0.535778, val accuracy: 0.517000\n",
      "Loss: 1.510614, Train accuracy: 0.537778, val accuracy: 0.528000\n",
      "Loss: 1.388365, Train accuracy: 0.537778, val accuracy: 0.525000\n",
      "Loss: 1.471337, Train accuracy: 0.538778, val accuracy: 0.525000\n",
      "Loss: 1.436905, Train accuracy: 0.540111, val accuracy: 0.520000\n",
      "Loss: 1.492498, Train accuracy: 0.542222, val accuracy: 0.522000\n",
      "Loss: 1.456445, Train accuracy: 0.543222, val accuracy: 0.526000\n",
      "Loss: 1.623928, Train accuracy: 0.544111, val accuracy: 0.529000\n",
      "Loss: 1.467945, Train accuracy: 0.546778, val accuracy: 0.526000\n",
      "Loss: 1.488047, Train accuracy: 0.544000, val accuracy: 0.523000\n",
      "Loss: 1.556411, Train accuracy: 0.549222, val accuracy: 0.525000\n",
      "Loss: 1.403959, Train accuracy: 0.549556, val accuracy: 0.523000\n",
      "Loss: 1.364535, Train accuracy: 0.548222, val accuracy: 0.522000\n",
      "Loss: 1.531124, Train accuracy: 0.550667, val accuracy: 0.525000\n",
      "Loss: 1.486443, Train accuracy: 0.552889, val accuracy: 0.524000\n",
      "Loss: 1.481838, Train accuracy: 0.549222, val accuracy: 0.529000\n",
      "Loss: 1.641446, Train accuracy: 0.552111, val accuracy: 0.535000\n",
      "Loss: 1.357263, Train accuracy: 0.552667, val accuracy: 0.528000\n",
      "Loss: 1.397876, Train accuracy: 0.553667, val accuracy: 0.539000\n",
      "Loss: 1.617847, Train accuracy: 0.556222, val accuracy: 0.536000\n",
      "Loss: 1.502198, Train accuracy: 0.555667, val accuracy: 0.535000\n",
      "Loss: 1.521280, Train accuracy: 0.559000, val accuracy: 0.541000\n",
      "Loss: 1.314376, Train accuracy: 0.559667, val accuracy: 0.537000\n",
      "Loss: 1.425346, Train accuracy: 0.560556, val accuracy: 0.547000\n",
      "Loss: 1.490057, Train accuracy: 0.560556, val accuracy: 0.539000\n",
      "Loss: 1.517067, Train accuracy: 0.561889, val accuracy: 0.547000\n",
      "Loss: 1.464706, Train accuracy: 0.564667, val accuracy: 0.546000\n",
      "Loss: 1.416632, Train accuracy: 0.565111, val accuracy: 0.547000\n",
      "Loss: 1.337836, Train accuracy: 0.564556, val accuracy: 0.545000\n",
      "Loss: 1.408251, Train accuracy: 0.566222, val accuracy: 0.543000\n",
      "Loss: 1.314289, Train accuracy: 0.565667, val accuracy: 0.548000\n",
      "Loss: 1.230305, Train accuracy: 0.567556, val accuracy: 0.547000\n",
      "Loss: 1.498915, Train accuracy: 0.566556, val accuracy: 0.545000\n",
      "Loss: 1.388989, Train accuracy: 0.567111, val accuracy: 0.551000\n",
      "Loss: 1.218006, Train accuracy: 0.568667, val accuracy: 0.545000\n",
      "Loss: 1.552638, Train accuracy: 0.570000, val accuracy: 0.550000\n",
      "Loss: 1.300269, Train accuracy: 0.569667, val accuracy: 0.550000\n",
      "Loss: 1.434734, Train accuracy: 0.571556, val accuracy: 0.550000\n",
      "Loss: 1.610923, Train accuracy: 0.571556, val accuracy: 0.549000\n",
      "Loss: 1.494486, Train accuracy: 0.571889, val accuracy: 0.558000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.384514, Train accuracy: 0.573222, val accuracy: 0.550000\n",
      "Loss: 1.388861, Train accuracy: 0.571667, val accuracy: 0.551000\n",
      "Loss: 1.495306, Train accuracy: 0.572556, val accuracy: 0.556000\n",
      "Loss: 1.273389, Train accuracy: 0.576556, val accuracy: 0.555000\n",
      "Loss: 1.400928, Train accuracy: 0.576333, val accuracy: 0.561000\n",
      "Loss: 1.234954, Train accuracy: 0.576222, val accuracy: 0.557000\n",
      "Loss: 1.421418, Train accuracy: 0.579111, val accuracy: 0.551000\n",
      "Loss: 1.410976, Train accuracy: 0.578111, val accuracy: 0.558000\n",
      "Loss: 1.351026, Train accuracy: 0.578222, val accuracy: 0.555000\n",
      "Loss: 1.423992, Train accuracy: 0.579889, val accuracy: 0.556000\n",
      "Loss: 1.358552, Train accuracy: 0.579556, val accuracy: 0.562000\n",
      "Loss: 1.323575, Train accuracy: 0.582000, val accuracy: 0.557000\n",
      "Loss: 1.245051, Train accuracy: 0.580000, val accuracy: 0.561000\n",
      "Loss: 1.488880, Train accuracy: 0.580333, val accuracy: 0.563000\n",
      "Loss: 1.231904, Train accuracy: 0.582556, val accuracy: 0.558000\n",
      "Loss: 1.409977, Train accuracy: 0.585667, val accuracy: 0.557000\n",
      "Loss: 1.283558, Train accuracy: 0.585556, val accuracy: 0.563000\n",
      "Loss: 1.386447, Train accuracy: 0.587000, val accuracy: 0.567000\n",
      "Loss: 1.563351, Train accuracy: 0.586111, val accuracy: 0.563000\n",
      "Loss: 1.374577, Train accuracy: 0.585111, val accuracy: 0.557000\n",
      "Loss: 1.377612, Train accuracy: 0.584000, val accuracy: 0.571000\n",
      "Loss: 1.233906, Train accuracy: 0.586111, val accuracy: 0.563000\n",
      "Loss: 1.361463, Train accuracy: 0.589778, val accuracy: 0.566000\n",
      "Loss: 1.303292, Train accuracy: 0.587444, val accuracy: 0.568000\n",
      "Loss: 1.262863, Train accuracy: 0.589222, val accuracy: 0.568000\n",
      "Loss: 1.392603, Train accuracy: 0.590667, val accuracy: 0.567000\n",
      "Loss: 1.238189, Train accuracy: 0.591889, val accuracy: 0.570000\n",
      "Loss: 1.339579, Train accuracy: 0.593444, val accuracy: 0.567000\n",
      "Loss: 1.206493, Train accuracy: 0.590444, val accuracy: 0.568000\n",
      "Loss: 1.244866, Train accuracy: 0.590778, val accuracy: 0.564000\n",
      "Loss: 1.189536, Train accuracy: 0.593000, val accuracy: 0.572000\n",
      "Loss: 1.338967, Train accuracy: 0.593444, val accuracy: 0.573000\n",
      "Loss: 1.351417, Train accuracy: 0.595556, val accuracy: 0.576000\n",
      "Loss: 1.245105, Train accuracy: 0.594333, val accuracy: 0.570000\n",
      "Loss: 1.268701, Train accuracy: 0.594778, val accuracy: 0.569000\n",
      "Loss: 1.241752, Train accuracy: 0.595444, val accuracy: 0.573000\n",
      "Loss: 1.490335, Train accuracy: 0.597889, val accuracy: 0.572000\n",
      "Loss: 1.344626, Train accuracy: 0.595333, val accuracy: 0.571000\n",
      "Loss: 1.390895, Train accuracy: 0.599556, val accuracy: 0.568000\n",
      "Loss: 1.548871, Train accuracy: 0.599333, val accuracy: 0.570000\n",
      "Loss: 1.327378, Train accuracy: 0.598889, val accuracy: 0.568000\n",
      "Loss: 1.302896, Train accuracy: 0.598111, val accuracy: 0.573000\n",
      "Loss: 1.297489, Train accuracy: 0.599000, val accuracy: 0.577000\n",
      "Loss: 1.531815, Train accuracy: 0.595556, val accuracy: 0.570000\n",
      "Loss: 1.336209, Train accuracy: 0.598556, val accuracy: 0.574000\n",
      "Loss: 1.188061, Train accuracy: 0.602222, val accuracy: 0.571000\n",
      "Loss: 1.502854, Train accuracy: 0.599444, val accuracy: 0.568000\n",
      "Loss: 1.404795, Train accuracy: 0.600889, val accuracy: 0.577000\n",
      "Loss: 1.331213, Train accuracy: 0.601333, val accuracy: 0.573000\n",
      "Loss: 1.347614, Train accuracy: 0.601889, val accuracy: 0.576000\n",
      "Loss: 1.391659, Train accuracy: 0.602667, val accuracy: 0.575000\n",
      "Loss: 1.393330, Train accuracy: 0.604000, val accuracy: 0.573000\n",
      "Loss: 1.434884, Train accuracy: 0.600111, val accuracy: 0.573000\n",
      "Loss: 1.287501, Train accuracy: 0.604111, val accuracy: 0.572000\n",
      "Loss: 1.280973, Train accuracy: 0.605889, val accuracy: 0.574000\n",
      "Loss: 1.293573, Train accuracy: 0.604556, val accuracy: 0.577000\n",
      "Loss: 1.275041, Train accuracy: 0.607000, val accuracy: 0.575000\n",
      "Loss: 1.421081, Train accuracy: 0.606333, val accuracy: 0.579000\n",
      "Loss: 1.299087, Train accuracy: 0.606556, val accuracy: 0.577000\n",
      "Loss: 1.344178, Train accuracy: 0.608333, val accuracy: 0.574000\n",
      "Loss: 1.262412, Train accuracy: 0.607000, val accuracy: 0.575000\n",
      "Loss: 1.299401, Train accuracy: 0.607444, val accuracy: 0.575000\n",
      "Loss: 1.471701, Train accuracy: 0.609778, val accuracy: 0.579000\n",
      "Loss: 1.324607, Train accuracy: 0.608222, val accuracy: 0.582000\n",
      "Loss: 1.320945, Train accuracy: 0.612111, val accuracy: 0.578000\n",
      "Loss: 1.421205, Train accuracy: 0.610778, val accuracy: 0.579000\n",
      "Loss: 1.296905, Train accuracy: 0.609222, val accuracy: 0.581000\n",
      "Loss: 1.351378, Train accuracy: 0.610222, val accuracy: 0.580000\n",
      "Loss: 1.350333, Train accuracy: 0.612889, val accuracy: 0.586000\n",
      "Loss: 1.028759, Train accuracy: 0.613889, val accuracy: 0.584000\n",
      "Loss: 1.302740, Train accuracy: 0.613444, val accuracy: 0.588000\n",
      "Loss: 1.175924, Train accuracy: 0.612333, val accuracy: 0.587000\n",
      "Loss: 1.320582, Train accuracy: 0.611444, val accuracy: 0.580000\n",
      "Loss: 1.385507, Train accuracy: 0.614444, val accuracy: 0.586000\n",
      "Loss: 1.248787, Train accuracy: 0.614000, val accuracy: 0.587000\n",
      "Loss: 1.233824, Train accuracy: 0.614222, val accuracy: 0.584000\n",
      "Loss: 1.471026, Train accuracy: 0.612667, val accuracy: 0.587000\n",
      "Loss: 1.198320, Train accuracy: 0.617222, val accuracy: 0.587000\n",
      "Loss: 1.409468, Train accuracy: 0.613889, val accuracy: 0.589000\n",
      "Loss: 1.234659, Train accuracy: 0.614444, val accuracy: 0.586000\n",
      "Loss: 1.268156, Train accuracy: 0.616667, val accuracy: 0.588000\n",
      "Loss: 1.482329, Train accuracy: 0.617778, val accuracy: 0.585000\n",
      "Loss: 1.403869, Train accuracy: 0.615667, val accuracy: 0.591000\n",
      "Loss: 1.277723, Train accuracy: 0.616333, val accuracy: 0.586000\n",
      "Loss: 1.216257, Train accuracy: 0.619556, val accuracy: 0.591000\n",
      "Loss: 1.175508, Train accuracy: 0.616444, val accuracy: 0.596000\n",
      "Loss: 1.300318, Train accuracy: 0.618333, val accuracy: 0.592000\n",
      "Loss: 1.397041, Train accuracy: 0.617000, val accuracy: 0.589000\n",
      "Loss: 1.236450, Train accuracy: 0.619333, val accuracy: 0.593000\n",
      "Loss: 1.206366, Train accuracy: 0.618333, val accuracy: 0.594000\n",
      "Loss: 1.428052, Train accuracy: 0.619556, val accuracy: 0.595000\n",
      "Loss: 1.166556, Train accuracy: 0.620000, val accuracy: 0.596000\n",
      "Loss: 1.385964, Train accuracy: 0.618778, val accuracy: 0.599000\n",
      "Loss: 1.429908, Train accuracy: 0.619556, val accuracy: 0.597000\n",
      "Loss: 1.262822, Train accuracy: 0.618778, val accuracy: 0.599000\n",
      "Loss: 1.425330, Train accuracy: 0.619333, val accuracy: 0.593000\n",
      "Loss: 1.170516, Train accuracy: 0.620333, val accuracy: 0.595000\n",
      "Loss: 1.246454, Train accuracy: 0.620333, val accuracy: 0.591000\n",
      "Loss: 1.372359, Train accuracy: 0.621000, val accuracy: 0.596000\n",
      "Loss: 1.270203, Train accuracy: 0.622000, val accuracy: 0.604000\n",
      "Loss: 1.384022, Train accuracy: 0.622778, val accuracy: 0.601000\n",
      "Loss: 1.215372, Train accuracy: 0.622778, val accuracy: 0.602000\n",
      "Loss: 1.253147, Train accuracy: 0.620667, val accuracy: 0.594000\n",
      "Loss: 1.226260, Train accuracy: 0.622111, val accuracy: 0.598000\n",
      "Loss: 1.276590, Train accuracy: 0.621667, val accuracy: 0.598000\n",
      "Loss: 1.371576, Train accuracy: 0.623444, val accuracy: 0.597000\n",
      "Loss: 1.249618, Train accuracy: 0.622889, val accuracy: 0.600000\n",
      "Loss: 1.301552, Train accuracy: 0.621556, val accuracy: 0.596000\n",
      "Loss: 1.285883, Train accuracy: 0.623778, val accuracy: 0.596000\n",
      "Loss: 1.487446, Train accuracy: 0.621444, val accuracy: 0.600000\n",
      "Loss: 1.411010, Train accuracy: 0.623222, val accuracy: 0.602000\n",
      "Loss: 1.498892, Train accuracy: 0.623444, val accuracy: 0.600000\n",
      "Loss: 1.269316, Train accuracy: 0.622778, val accuracy: 0.597000\n",
      "Loss: 1.210881, Train accuracy: 0.625000, val accuracy: 0.599000\n",
      "Loss: 1.264412, Train accuracy: 0.624556, val accuracy: 0.600000\n",
      "Loss: 1.272763, Train accuracy: 0.625667, val accuracy: 0.602000\n",
      "Loss: 1.155821, Train accuracy: 0.624000, val accuracy: 0.608000\n",
      "Loss: 1.417262, Train accuracy: 0.622667, val accuracy: 0.604000\n",
      "Loss: 1.328120, Train accuracy: 0.624444, val accuracy: 0.601000\n",
      "Loss: 1.142428, Train accuracy: 0.624778, val accuracy: 0.600000\n",
      "Loss: 1.396339, Train accuracy: 0.624667, val accuracy: 0.602000\n",
      "Loss: 1.284393, Train accuracy: 0.625444, val accuracy: 0.604000\n",
      "Loss: 1.383993, Train accuracy: 0.625333, val accuracy: 0.601000\n",
      "Loss: 1.468651, Train accuracy: 0.625778, val accuracy: 0.603000\n",
      "Loss: 1.210074, Train accuracy: 0.625556, val accuracy: 0.602000\n",
      "Loss: 1.174888, Train accuracy: 0.627000, val accuracy: 0.600000\n",
      "Loss: 1.231031, Train accuracy: 0.625667, val accuracy: 0.602000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.206839, Train accuracy: 0.626111, val accuracy: 0.602000\n",
      "Loss: 1.338205, Train accuracy: 0.625889, val accuracy: 0.604000\n",
      "Loss: 1.293178, Train accuracy: 0.624778, val accuracy: 0.604000\n",
      "Loss: 1.334126, Train accuracy: 0.626667, val accuracy: 0.604000\n",
      "Loss: 1.342318, Train accuracy: 0.627333, val accuracy: 0.604000\n",
      "Loss: 1.359121, Train accuracy: 0.625222, val accuracy: 0.601000\n",
      "Loss: 1.338200, Train accuracy: 0.627000, val accuracy: 0.606000\n",
      "Loss: 1.275602, Train accuracy: 0.627667, val accuracy: 0.604000\n",
      "Loss: 1.348156, Train accuracy: 0.628000, val accuracy: 0.603000\n",
      "Loss: 1.325405, Train accuracy: 0.628778, val accuracy: 0.606000\n",
      "Loss: 1.181102, Train accuracy: 0.629444, val accuracy: 0.607000\n",
      "Loss: 1.104719, Train accuracy: 0.627889, val accuracy: 0.606000\n",
      "Loss: 1.196017, Train accuracy: 0.628667, val accuracy: 0.605000\n",
      "Loss: 1.256067, Train accuracy: 0.628111, val accuracy: 0.608000\n",
      "Loss: 1.224062, Train accuracy: 0.628444, val accuracy: 0.608000\n",
      "Loss: 1.315619, Train accuracy: 0.628667, val accuracy: 0.604000\n",
      "Loss: 1.271700, Train accuracy: 0.628889, val accuracy: 0.606000\n",
      "Loss: 1.263473, Train accuracy: 0.628111, val accuracy: 0.605000\n",
      "Loss: 1.239327, Train accuracy: 0.630778, val accuracy: 0.605000\n",
      "Loss: 1.292143, Train accuracy: 0.629778, val accuracy: 0.608000\n",
      "Loss: 1.330983, Train accuracy: 0.628000, val accuracy: 0.603000\n",
      "Loss: 1.318564, Train accuracy: 0.631222, val accuracy: 0.601000\n",
      "Loss: 1.235344, Train accuracy: 0.630778, val accuracy: 0.609000\n",
      "Loss: 1.193146, Train accuracy: 0.630778, val accuracy: 0.608000\n",
      "Loss: 1.160424, Train accuracy: 0.632111, val accuracy: 0.607000\n",
      "Loss: 1.116878, Train accuracy: 0.629222, val accuracy: 0.611000\n",
      "Loss: 1.264162, Train accuracy: 0.631667, val accuracy: 0.606000\n",
      "Loss: 1.319596, Train accuracy: 0.630556, val accuracy: 0.609000\n",
      "Loss: 1.385552, Train accuracy: 0.631778, val accuracy: 0.610000\n",
      "Loss: 1.230002, Train accuracy: 0.631556, val accuracy: 0.608000\n",
      "Loss: 1.062426, Train accuracy: 0.631000, val accuracy: 0.609000\n",
      "Loss: 1.238435, Train accuracy: 0.630000, val accuracy: 0.609000\n",
      "Loss: 1.387329, Train accuracy: 0.631889, val accuracy: 0.607000\n",
      "Loss: 1.274858, Train accuracy: 0.631778, val accuracy: 0.610000\n",
      "Loss: 1.231304, Train accuracy: 0.632333, val accuracy: 0.612000\n",
      "Loss: 1.350619, Train accuracy: 0.631000, val accuracy: 0.609000\n",
      "Loss: 1.125159, Train accuracy: 0.630444, val accuracy: 0.606000\n",
      "Loss: 1.216832, Train accuracy: 0.632222, val accuracy: 0.609000\n",
      "Loss: 1.053467, Train accuracy: 0.631667, val accuracy: 0.609000\n",
      "Loss: 1.368077, Train accuracy: 0.632000, val accuracy: 0.609000\n",
      "Loss: 1.117864, Train accuracy: 0.632667, val accuracy: 0.610000\n",
      "Loss: 1.274291, Train accuracy: 0.632778, val accuracy: 0.610000\n",
      "Loss: 1.204414, Train accuracy: 0.631667, val accuracy: 0.607000\n",
      "Loss: 1.250282, Train accuracy: 0.633111, val accuracy: 0.613000\n",
      "Loss: 1.140558, Train accuracy: 0.634111, val accuracy: 0.614000\n",
      "Loss: 1.076310, Train accuracy: 0.634000, val accuracy: 0.615000\n",
      "Loss: 1.148564, Train accuracy: 0.632333, val accuracy: 0.610000\n",
      "Loss: 1.264801, Train accuracy: 0.635111, val accuracy: 0.606000\n",
      "Loss: 1.166009, Train accuracy: 0.633444, val accuracy: 0.610000\n",
      "Loss: 1.317580, Train accuracy: 0.633889, val accuracy: 0.611000\n",
      "Loss: 1.414238, Train accuracy: 0.633111, val accuracy: 0.611000\n",
      "Loss: 1.353886, Train accuracy: 0.634889, val accuracy: 0.615000\n",
      "Loss: 1.410717, Train accuracy: 0.634333, val accuracy: 0.611000\n",
      "Loss: 1.391024, Train accuracy: 0.635222, val accuracy: 0.612000\n",
      "Loss: 1.324906, Train accuracy: 0.634889, val accuracy: 0.611000\n",
      "Loss: 1.187064, Train accuracy: 0.633000, val accuracy: 0.612000\n",
      "Loss: 1.270568, Train accuracy: 0.633333, val accuracy: 0.608000\n",
      "Loss: 1.183292, Train accuracy: 0.636111, val accuracy: 0.610000\n",
      "Loss: 1.266201, Train accuracy: 0.635778, val accuracy: 0.612000\n",
      "Loss: 1.192627, Train accuracy: 0.634111, val accuracy: 0.612000\n",
      "Loss: 1.272380, Train accuracy: 0.635778, val accuracy: 0.611000\n",
      "Loss: 1.173838, Train accuracy: 0.636778, val accuracy: 0.612000\n",
      "Loss: 1.347869, Train accuracy: 0.636222, val accuracy: 0.613000\n",
      "Loss: 1.214793, Train accuracy: 0.635667, val accuracy: 0.615000\n",
      "Loss: 1.228073, Train accuracy: 0.634778, val accuracy: 0.608000\n",
      "Loss: 1.175929, Train accuracy: 0.637000, val accuracy: 0.615000\n",
      "Loss: 1.263897, Train accuracy: 0.635556, val accuracy: 0.612000\n",
      "Loss: 1.175805, Train accuracy: 0.634556, val accuracy: 0.610000\n",
      "Loss: 1.135187, Train accuracy: 0.634778, val accuracy: 0.612000\n",
      "Loss: 1.177786, Train accuracy: 0.635889, val accuracy: 0.611000\n",
      "Loss: 1.041290, Train accuracy: 0.636556, val accuracy: 0.610000\n",
      "Loss: 1.127482, Train accuracy: 0.636333, val accuracy: 0.613000\n",
      "Loss: 1.177962, Train accuracy: 0.635778, val accuracy: 0.611000\n",
      "Loss: 1.151314, Train accuracy: 0.634667, val accuracy: 0.610000\n",
      "Loss: 1.244348, Train accuracy: 0.636222, val accuracy: 0.613000\n",
      "Loss: 1.146953, Train accuracy: 0.637222, val accuracy: 0.609000\n",
      "Loss: 1.211169, Train accuracy: 0.636778, val accuracy: 0.608000\n",
      "Loss: 1.341877, Train accuracy: 0.638333, val accuracy: 0.611000\n",
      "Loss: 1.473650, Train accuracy: 0.638444, val accuracy: 0.615000\n",
      "Loss: 1.111988, Train accuracy: 0.638556, val accuracy: 0.616000\n",
      "Loss: 1.248903, Train accuracy: 0.638000, val accuracy: 0.613000\n",
      "Loss: 1.326535, Train accuracy: 0.637667, val accuracy: 0.613000\n",
      "Loss: 1.154007, Train accuracy: 0.638222, val accuracy: 0.614000\n",
      "Loss: 1.310678, Train accuracy: 0.636778, val accuracy: 0.612000\n",
      "Loss: 1.214982, Train accuracy: 0.640556, val accuracy: 0.615000\n",
      "Loss: 1.130345, Train accuracy: 0.638444, val accuracy: 0.610000\n",
      "Loss: 1.328364, Train accuracy: 0.639222, val accuracy: 0.613000\n",
      "Loss: 1.099766, Train accuracy: 0.640444, val accuracy: 0.616000\n",
      "Loss: 1.199940, Train accuracy: 0.639778, val accuracy: 0.616000\n",
      "Loss: 1.179291, Train accuracy: 0.639000, val accuracy: 0.616000\n",
      "Loss: 1.029151, Train accuracy: 0.639333, val accuracy: 0.617000\n",
      "Loss: 1.323931, Train accuracy: 0.640111, val accuracy: 0.620000\n",
      "Loss: 1.155780, Train accuracy: 0.639333, val accuracy: 0.619000\n",
      "Loss: 1.205796, Train accuracy: 0.640111, val accuracy: 0.617000\n",
      "Loss: 1.000915, Train accuracy: 0.639667, val accuracy: 0.613000\n",
      "Loss: 1.356268, Train accuracy: 0.638889, val accuracy: 0.616000\n",
      "Loss: 1.143448, Train accuracy: 0.640000, val accuracy: 0.616000\n",
      "Loss: 1.384473, Train accuracy: 0.638111, val accuracy: 0.618000\n",
      "Loss: 1.590033, Train accuracy: 0.641111, val accuracy: 0.617000\n",
      "Loss: 1.016425, Train accuracy: 0.637667, val accuracy: 0.616000\n",
      "Loss: 1.227767, Train accuracy: 0.641778, val accuracy: 0.619000\n",
      "Loss: 1.144690, Train accuracy: 0.639667, val accuracy: 0.616000\n",
      "Loss: 1.108125, Train accuracy: 0.639333, val accuracy: 0.616000\n",
      "Loss: 1.139011, Train accuracy: 0.642222, val accuracy: 0.616000\n",
      "Loss: 1.103701, Train accuracy: 0.641889, val accuracy: 0.618000\n",
      "Loss: 1.081896, Train accuracy: 0.640111, val accuracy: 0.617000\n",
      "Loss: 1.166549, Train accuracy: 0.641000, val accuracy: 0.622000\n",
      "Loss: 1.178448, Train accuracy: 0.640667, val accuracy: 0.617000\n",
      "Loss: 1.079701, Train accuracy: 0.643000, val accuracy: 0.617000\n",
      "Loss: 1.247245, Train accuracy: 0.642889, val accuracy: 0.619000\n",
      "Loss: 1.446071, Train accuracy: 0.640889, val accuracy: 0.615000\n",
      "Loss: 1.135242, Train accuracy: 0.642778, val accuracy: 0.621000\n",
      "Loss: 1.435063, Train accuracy: 0.642889, val accuracy: 0.616000\n",
      "Loss: 1.236932, Train accuracy: 0.642556, val accuracy: 0.617000\n",
      "Loss: 1.124144, Train accuracy: 0.643000, val accuracy: 0.620000\n",
      "Loss: 1.201540, Train accuracy: 0.640889, val accuracy: 0.616000\n",
      "Loss: 1.314362, Train accuracy: 0.642889, val accuracy: 0.617000\n",
      "Loss: 1.184062, Train accuracy: 0.641556, val accuracy: 0.616000\n",
      "Loss: 1.120403, Train accuracy: 0.643667, val accuracy: 0.616000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-0162ad3476cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-2\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 256\n",
    "num_epochs = 500\n",
    "batch_size = 100\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = hidden_layer_size, \n",
    "                    reg = reg_strength)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=learning_rates, num_epochs=num_epochs, \n",
    "                  batch_size=batch_size, learning_rate_decay=learning_rate_decay)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb18a21a58>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.564000\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
